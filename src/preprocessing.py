import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import os
import joblib

def preprocess_data(data_path):
    """
    Effectue le prÃ©traitement complet des donnÃ©es :
    - Chargement
    - Nettoyage
    - Encodage
    - Normalisation
    - Split train/test
    
    Sauvegarde Ã©galement le scaler et l'encoder pour utilisation future
    """
    print("ğŸ“‚ Chargement des donnÃ©es...")
    df = pd.read_csv(data_path)
    print(f"   âœ“ {df.shape[0]} lignes et {df.shape[1]} colonnes chargÃ©es")
    
    # VÃ©rification des valeurs manquantes
    missing_before = df.isnull().sum().sum()
    if missing_before > 0:
        print(f"âš ï¸  {missing_before} valeurs manquantes dÃ©tectÃ©es")
        df = df.dropna()
        print(f"   âœ“ Lignes supprimÃ©es, nouveau total: {df.shape[0]}")
    else:
        print("   âœ“ Aucune valeur manquante")
    
    # VÃ©rification des doublons
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        print(f"âš ï¸  {duplicates} doublons dÃ©tectÃ©s")
        df = df.drop_duplicates()
        print(f"   âœ“ Doublons supprimÃ©s, nouveau total: {df.shape[0]}")
    else:
        print("   âœ“ Aucun doublon")

    # Encodage de la variable cible
    print("\nğŸ”„ Encodage de la variable cible (stress_level)...")
    encoder = LabelEncoder()
    df['stress_level'] = encoder.fit_transform(df['stress_level'])
    print(f"   âœ“ Classes: {encoder.classes_}")
    print(f"   âœ“ Distribution aprÃ¨s encodage:")
    for class_label, encoded_value in zip(encoder.classes_, range(len(encoder.classes_))):
        count = (df['stress_level'] == encoded_value).sum()
        print(f"      - Classe {class_label} â†’ {encoded_value} ({count} Ã©chantillons)")

    # SÃ©paration X / y
    print("\nâœ‚ï¸  SÃ©paration des features et de la cible...")
    X = df.drop('stress_level', axis=1)
    y = df['stress_level']
    print(f"   âœ“ X shape: {X.shape}")
    print(f"   âœ“ y shape: {y.shape}")

    # Encodage des variables catÃ©gorielles (si prÃ©sentes)
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
    if categorical_cols:
        print(f"\nğŸ”¤ Encodage des variables catÃ©gorielles: {categorical_cols}")
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
        print(f"   âœ“ Nouvelles features aprÃ¨s encodage: {X.shape[1]}")
    else:
        print("\n   â„¹ï¸  Aucune variable catÃ©gorielle Ã  encoder")

    # Normalisation
    print("\nğŸ“Š Normalisation des donnÃ©es (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print(f"   âœ“ Moyenne aprÃ¨s scaling: {X_scaled.mean():.6f}")
    print(f"   âœ“ Ã‰cart-type aprÃ¨s scaling: {X_scaled.std():.6f}")

    # Train / Test split
    print("\nğŸ² Division train/test (80/20)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   âœ“ Train set: {X_train.shape[0]} Ã©chantillons ({X_train.shape[0]/len(X_scaled)*100:.1f}%)")
    print(f"   âœ“ Test set:  {X_test.shape[0]} Ã©chantillons ({X_test.shape[0]/len(X_scaled)*100:.1f}%)")
    
    # Distribution des classes dans train/test
    print("\nğŸ“ˆ Distribution des classes:")
    print("   Train:")
    for class_val in np.unique(y_train):
        count = (y_train == class_val).sum()
        print(f"      - Classe {class_val}: {count} ({count/len(y_train)*100:.1f}%)")
    print("   Test:")
    for class_val in np.unique(y_test):
        count = (y_test == class_val).sum()
        print(f"      - Classe {class_val}: {count} ({count/len(y_test)*100:.1f}%)")
    
    # Sauvegarde du scaler et de l'encoder pour utilisation future
    # CORRECTION : Utiliser le chemin absolu
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    models_dir = os.path.join(project_root, "results", "models")
    
    # IMPORTANT : CrÃ©er le dossier s'il n'existe pas
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
        print(f"\nğŸ“ Dossier crÃ©Ã© : {models_dir}")
    
    scaler_path = os.path.join(models_dir, "scaler.pkl")
    encoder_path = os.path.join(models_dir, "label_encoder.pkl")
    
    joblib.dump(scaler, scaler_path)
    joblib.dump(encoder, encoder_path)
    print(f"\nğŸ’¾ Scaler sauvegardÃ©: {scaler_path}")
    print(f"ğŸ’¾ Encoder sauvegardÃ©: {encoder_path}")

    return X_train, X_test, y_train, y_test, X.columns.tolist()